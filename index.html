<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Wancong Zhang</title>

    <meta name="author" content="Wancong Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Wancong (Kevin) Zhang
                </p>
                <p>I am a first year PhD student at <a href="https://cims.nyu.edu/dynamic/">NYU Courant</a>, specializing in deep learning under the guidance of <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a>.
                </p>
                
                <p>
                  Prior to my PhD, I worked as a senior researcher at <a href="https://www.assemblyai.com/">AssemblyAI</a>, where I led the developments of <a href="https://www.assemblyai.com/blog/conformer-1/">Conformer-1</a>, <a href="https://www.assemblyai.com/blog/conformer-2/">Conformer-2</a>, and realtime automatic speech recognition (ASR) models. I earned my M.S. degree from NYU Courant Institute's Computer Science Department, where I had the privilege to collaborate with <a href="https://scholar.google.com/citations?user=h8u3ll8AAAAJ&hl=fr">Nicolas Carion</a>, <a href="https://healthyml.org/marzyeh/">Marzyeh Ghassemi</a>, and <a href="https://cims.nyu.edu/~rajeshr/">Rajesh Ranganath</a>. Before starting my AI journey, I explored the world of stem cell and molecular biology research at <a href="https://hsci.harvard.edu/">Harvard Stem Cell Institute</a>.
                </p>

                <p style="text-align:center">
                  <a href="mailto:wz1232@nyu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Jc0cRkUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/kevinghst/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/kevin_bio_circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/kevin_bio_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research (Updated 06/2024)</h2>
                <p>
                  I believe the ability to learn from observations and interacting with the world in an unsupervised manner is a crucial stepping stone to unlocking general intelligence. My primary research focus revolves around self supervised learning (SSL) and novelty guided explorations, with an emphasis on their applications in computer vision and control. 
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
      
      <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <!-- <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/owl.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div> -->
            <img src='images/lightweight_probe.png' width="160">
          </div>
          <script type="text/javascript">
            function db3d_start() {
              document.getElementById('db3d_image').style.opacity = "1";
            }

            function db3d_stop() {
              document.getElementById('db3d_image').style.opacity = "0";
            }
            db3d_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2208.12345.pdf">
            <span class="papertitle">Light-weight probing of unsupervised representations for Reinforcement Learning</span>
          </a>
          <br>
          <strong>Wancong Zhang</strong>, <a href="https://scholar.google.com/citations?user=7jAlFsIAAAAJ&hl=en">Anthony GX-Chen</a>, <a href="https://scholar.google.com/citations?user=uRg4vX8AAAAJ&hl=en">Vlad Sobal</a>, <a href="https://scholar.google.com/citations?user=WLN3QrAAAAAJ&hl=en">Yann LeCun</a>, <a href="https://scholar.google.com/citations?user=h8u3ll8AAAAJ&hl=fr">Nicolas Carion</a>
          <br>
          <a href="https://github.com/kevinghst/lightweight_rl_probe">code</a> / 
          <a href="https://arxiv.org/pdf/2208.12345.pdf">arXiv</a>
          <br>
          <strong><em>Reinforcement Learning Conference</em>, 2024</strong>
          <p></p>
          <p>Presents an efficient probing benchmark to evaluate the fitness of unsupervised visual representations for reinforcement learning (RL). Applied it to systematically improve pre-existing SSL recipes for RL.</p>
        </td>
      </tr>

      <tr onmouseout="db3d_stop()" onmouseover="db3d_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <!-- <div class="two" id='db3d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/owl.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div> -->
            <img src='images/conformer-1.png' width="160">
          </div>
          <script type="text/javascript">
            function db3d_start() {
              document.getElementById('db3d_image').style.opacity = "1";
            }

            function db3d_stop() {
              document.getElementById('db3d_image').style.opacity = "0";
            }
            db3d_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2404.07341.pdf">
            <span class="papertitle">Conformer-1: Robust ASR via Large-Scale Semisupervised Bootstrapping</span>
          </a>
          <br>
          <strong>Wancong Zhang*</strong>, Luka Chkhetiani*, Francis McCann Ramirez*, Yash Khare, Andrea Vanzo, Michael Liang, Sergio Ramirez Martin, Gabriel Oexle, Ruben Rousbib,  Taufiquzzaman Peyash, Michael Nguyen, Dillon Pulliam, <a href="https://scholar.google.com/citations?user=VQipQCgAAAAJ&hl=en&oi=ao">Domenic Donato</a>
          <br>
          <a href="https://arxiv.org/pdf/2404.07341.pdf">arXiv</a>
          <p></p>
          <p>Showcases an industrial-scale end-to-end Automatic Speech Recognition model trained on 570k hours of speech audio data using Noisy Student. It achieves competitive word error rates against larger and more computationally expensive models.</p>
        </td>
      </tr>

      <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/mixup.png' width="160">
          </div>
          <script type="text/javascript">
            function bakedsdf_start() {
              document.getElementById('bakedsdf_image').style.opacity = "1";
            }

            function bakedsdf_stop() {
              document.getElementById('bakedsdf_image').style.opacity = "0";
            }
            bakedsdf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2102.11402.pdf">
            <span class="papertitle">MixUp Training Leads to Reduced Overfitting and Improved Calibration for the Transformer Architecture
            </span>
          </a>
          <br>
          <strong>Wancong Zhang</strong>,
          <a href="https://github.com/ieshanvaidya">Ieshan Vaidya</a>
          <br>
          <a href="https://arxiv.org/pdf/2102.11402.pdf">arXiv</a>
          <p></p>
          <p>
            Adapts the computer vision data augmentation technique MixUp to the natural language domain, reducing calibration error of transformers for sentence classification by up to 50%.
          </p>
        </td>
      </tr>


      <tr onmouseout="merf_stop()" onmouseover="merf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/chil.png' width="160">
          </div>
          <script type="text/javascript">
            function merf_start() {
              document.getElementById('merf_image').style.opacity = "1";
            }

            function merf_stop() {
              document.getElementById('merf_image').style.opacity = "0";
            }
            merf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dl.acm.org/doi/pdf/10.1145/3450439.3451877">
            <span class="papertitle">A comprehensive EHR timeseries pre-training benchmark</span>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=_V96PXoAAAAJ&hl=en/">Matthew McDermott</a>,
          <a href="https://scholar.google.ca/citations?user=lVxcdRAAAAAJ&hl=en">Bret Nestor</a>,
          <a href="https://scholar.google.com/citations?user=XvNxmJIAAAAJ&hl=en">Evan Kim</a>,
          <strong>Wancong Zhang</strong>,
          <a href="https://scholar.google.com/citations?user=cEepZOEAAAAJ&hl=en">Anna Goldenberg</a>, <br>
          <a href="https://scholar.google.com/citations?user=1LuGqFQAAAAJ&hl=en">Peter Szolovitz</a>,
          <a href="https://scholar.google.com/citations?user=9RyeFYwAAAAJ&hl=en">Marzyeh Ghassemi</a>
          <br>
          <a href="https://dl.acm.org/doi/pdf/10.1145/3450439.3451877">arXiv</a>
          <br>
          <strong><em>ACM CHIL</em>, 2021</strong>
          <p></p>
          <p>
          Establishes a pre-training benchmark protocol for electronic health record (EHR) data.
          </p>
        </td>
      </tr>

          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody></tbody>
            <tr>
              <td>
                <h2>Teaching</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/deeplearning_nyu.jpg" width="120">
            </td>
            <td width="75%" valign="center">
              <a href="https://atcold.github.io/NYU-DLSP21/">Deep Learning (DS-GA 1008), Fall 2024</a>
            </td>
          </tr>

        </td>
      </tr>
    </table>
  </body>
</html>
